import { RemoteLLMComponent } from '../../components/remote_llm_component';
import { Camelize } from '../../constants';
import { Node as GraphConfigNode, TextGenerationConfig } from '../../graph_config_schema';
import { AbstractNode, AbstractNodeProps } from './abstract_node';
/**
 * Configuration for RemoteLLMCompletionNode using LLM provider settings.
 * This approach creates both the node and the underlying LLM component.
 * Provider and modelName are optional and will use defaults if not specified.
 */
interface RemoteLLMCompletionNodeProps extends AbstractNodeProps {
    /** Text generation configuration parameters */
    textGenerationConfig?: Camelize<TextGenerationConfig>;
    /** Whether to stream responses */
    stream?: boolean;
    /** LLM provider (e.g., 'openai', 'anthropic', 'inworld') */
    provider?: string;
    /** Model name specific to the provider (e.g., 'gpt-4', 'claude-3-5-sonnet-20241022') */
    modelName?: string;
}
/**
 * Configuration for RemoteLLMCompletionNode using an existing LLM component.
 * This approach references a pre-configured LLM component that could be reused across multiple nodes.
 */
interface RemoteLLMCompletionNodeWithLLMComponentProps extends AbstractNodeProps {
    /** Existing LLM component to use */
    llmComponent: RemoteLLMComponent;
    /** Text generation configuration parameters */
    textGenerationConfig?: Camelize<TextGenerationConfig>;
    /** Whether to stream responses */
    stream?: boolean;
}
/**
 * Remote LLM completion node for text generation.
 * Use a pre-configured LLM component (reusable across nodes), or provide provider/model
 * details and the node will create the component for you.
 *
 * @input {String} {@link String} - The data type that LLMCompletionNode accepts as input
 * @output {String} {@link String} - The data type that LLMCompletionNode outputs
 *
 * @example
 * ```typescript
 * // Using LLM provider configuration
 * const completionNode = new RemoteLLMCompletionNode({
 *   id: 'my-completion-node',
 *   provider: 'openai',
 *   modelName: 'gpt-4o-mini',
 *   stream: true
 * });
 *
 * // Using existing LLM component
 * const completionNodeWithComponent = new RemoteLLMCompletionNode({
 *   id: 'my-completion-node',
 *   llmComponent: existingLLMComponent
 * });
 *
 * // Using default settings
 * const defaultCompletionNode = new RemoteLLMCompletionNode();
 * ```
 */
export declare class RemoteLLMCompletionNode extends AbstractNode {
    private executionConfig;
    /**
     * Creates a new RemoteLLMCompletionNode instance.
     *
     * @param props - Optional configuration for the LLM completion node. Can specify either LLM provider settings
     *                or reference an existing LLM component, but not both. If not provided, uses default settings.
     */
    constructor(props?: RemoteLLMCompletionNodeProps | RemoteLLMCompletionNodeWithLLMComponentProps);
    protected toGraphConfigNode(): GraphConfigNode;
}
export {};
