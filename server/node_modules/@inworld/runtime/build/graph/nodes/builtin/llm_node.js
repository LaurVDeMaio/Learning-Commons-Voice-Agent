"use strict";
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.LLMNode = void 0;
const string_1 = require("../../../common/stream/string");
const expose_binary_1 = require("../../../expose_binary");
const llm_1 = require("../../../primitives/llm");
const data_types_1 = require("../../data_types");
const node_1 = require("../node");
/**
 * Node for language model text generation in the graph.
 * Processes input through a language model to generate text responses.
 */
class LLMNode extends node_1.Node {
    /**
     * Creates a new LLMNode instance.
     *
     * @param props - LLM node creation properties
     * @internal
     */
    constructor(props) {
        let config;
        try {
            config = new llm_1.TextGenerationConfig(props.textGenerationConfig);
            super(props.id, expose_binary_1.NodeHelperFunctions.createLLMNode(props.id, props.llm.getExternal(), config.getExternal(), props.stream));
            this.llm = props.llm;
            this.stream = props.stream;
        }
        finally {
            config === null || config === void 0 ? void 0 : config.destroy();
        }
    }
    /**
     * Creates a response handler for processing graph execution results.
     * Handles both streaming and non-streaming modes.
     *
     * @param graphExecutor - Reference to the graph executor
     * @param index - Index of the execution
     * @returns Object with the next function for handling stream data
     */
    handleResponse(graphExecutor, index) {
        return __awaiter(this, void 0, void 0, function* () {
            if (this.stream) {
                if (index && (yield graphExecutor.hasNext(index))) {
                    const next = yield graphExecutor.next(index);
                    this.checkStatus(next);
                    return {
                        next: this.nextStreaming.bind(this, next),
                    };
                }
                return {
                    next: () => Promise.resolve({ done: true }),
                };
            }
            return {
                next: this.next.bind(this, index, graphExecutor),
            };
        });
    }
    /**
     * Cleans up resources associated with this LLM node.
     */
    destroy() {
        var _a;
        (_a = this.llm) === null || _a === void 0 ? void 0 : _a.destroy();
        if (this.external) {
            expose_binary_1.NodeHelperFunctions.deleteLLMNode(this.external);
            super.destroy();
        }
    }
    /**
     * Processes the next chunk of data in non-streaming mode.
     *
     * @param index - Index of the execution
     * @param graphExecutor - Reference to the graph executor
     * @returns Promise resolving to the next result with text or done flag
     */
    next(index, graphExecutor) {
        return __awaiter(this, void 0, void 0, function* () {
            while (index && (yield graphExecutor.hasNext(index))) {
                const next = yield graphExecutor.next(index);
                this.checkStatus(next);
                let text = '';
                let textData;
                try {
                    textData = data_types_1.TextData.fromExternal(next);
                    text = textData.getText();
                }
                finally {
                    textData === null || textData === void 0 ? void 0 : textData.destroy();
                }
                return {
                    text,
                    done: false,
                };
            }
            return {
                done: true,
            };
        });
    }
    /**
     * Processes the next chunk of data in streaming mode.
     *
     * @param next - Reference to the next data chunk
     * @returns Promise resolving to the streamed text data
     */
    nextStreaming(next) {
        return __awaiter(this, void 0, void 0, function* () {
            const stream = expose_binary_1.StreamStringFunctions.toStream(next);
            const streamData = expose_binary_1.StreamStringFunctions.getStream(stream);
            const stringStream = new string_1.StringStream(streamData, () => {
                expose_binary_1.StreamStringFunctions.delete(stream);
                expose_binary_1.StreamStringFunctions.deleteStream(streamData);
            });
            return stringStream.next();
        });
    }
}
exports.LLMNode = LLMNode;
