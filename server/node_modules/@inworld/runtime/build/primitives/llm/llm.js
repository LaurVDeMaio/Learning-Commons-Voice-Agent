"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.LLM = void 0;
const common_1 = require("../../common");
const helpers_1 = require("../../common/helpers");
const content_1 = require("../../common/stream/content");
const messages_1 = require("../../common/vector/messages");
const expose_binary_1 = require("../../expose_binary");
const messages_2 = require("./messages");
const text_generator_1 = require("./text_generator");
/**
 * Class for interacting with Language Models (LLMs).
 * Provides functionality to generate text using prompts or message sequences.
 */
class LLM {
    /**
     * Creates a new LLM instance.
     *
     * @param llm - External reference to the native LLM implementation
     * @param factory - External reference to the LLM factory
     * @internal
     */
    constructor(llm, factory) {
        this.llm = llm;
        this.factory = factory;
    }
    /**
     * Generates text based on a prompt or message sequence.
     * This method now uses ContentStream to be consistent with other LLM methods.
     *
     * @param props - Configuration for text generation
     * @param props.prompt - Optional text prompt to use for generation
     * @param props.messages - Optional array of message objects for chat-like interactions
     * @param props.config - Configuration parameters for text generation
     * @returns A content stream iterator for the generated content (text + potential tool calls)
     * @throws InworldError if neither prompt nor messages are provided, or generation fails
     */
    async generateText(props) {
        if (!props.prompt && !props.messages) {
            throw new common_1.InworldError('Either prompt or messages must be provided for LLM generation');
        }
        let generateStatus;
        let messages = null;
        try {
            if (props.messages) {
                messages = messages_1.Messages.fromLLMMessageInterfaces(props.messages);
            }
            generateStatus = await this.buildGenerateStatus({
                prompt: props.prompt,
                messages: messages,
                config: props.config,
            });
            if (!expose_binary_1.LLMInterfaceFunctions.isOK(generateStatus)) {
                throw new common_1.InworldError('Failed to generate text', (0, helpers_1.getStatus)(generateStatus));
            }
            return this.contentStreamIterator(expose_binary_1.InputStreamFunctions.get(generateStatus), messages);
        }
        catch (error) {
            messages === null || messages === void 0 ? void 0 : messages.destroy();
            throw error;
        }
        finally {
            if (generateStatus) {
                expose_binary_1.InputStreamFunctions.delete(generateStatus);
            }
        }
    }
    /**
     * Generates content based on messages with optional tool calling support.
     * This is the advanced API that supports structured content with tool calls.
     *
     * @param props - Configuration for content generation
     * @param props.messages - Array of message objects for chat-like interactions
     * @param props.config - Configuration parameters for text generation
     * @param props.tools - Optional array of tools that the model may call
     * @returns A stream iterator for the generated content (text + tool calls)
     * @throws InworldError if messages are not provided, or generation fails
     */
    async generateContent(props) {
        if (!props.messages) {
            throw new common_1.InworldError('Messages must be provided for LLM content generation');
        }
        let generateStatus;
        let messages = null;
        let tools = null;
        try {
            messages = messages_1.Messages.fromLLMMessageInterfaces(props.messages);
            if (props.tools && props.tools.length > 0) {
                tools = messages_2.Tools.fromToolInterfaces(props.tools);
                generateStatus =
                    await expose_binary_1.LLMInterfaceFunctions.generateContentFromMessagesWithTools(this.llm, messages.getExternal(), new text_generator_1.TextGenerationConfig(props.config).getExternal(), tools.getExternal());
            }
            else {
                generateStatus = await this.buildGenerateStatus({
                    messages: messages,
                    config: props.config,
                });
            }
            if (!expose_binary_1.LLMInterfaceFunctions.isOK(generateStatus)) {
                throw new common_1.InworldError('Failed to generate content', (0, helpers_1.getStatus)(generateStatus));
            }
            return this.contentStreamIterator(expose_binary_1.InputStreamFunctions.get(generateStatus), messages);
        }
        catch (error) {
            messages === null || messages === void 0 ? void 0 : messages.destroy();
            tools === null || tools === void 0 ? void 0 : tools.destroy();
            throw error;
        }
        finally {
            if (generateStatus) {
                expose_binary_1.InputStreamFunctions.delete(generateStatus);
            }
        }
    }
    /**
     * Generates content stream based on messages with optional tool calling support.
     * This method returns a content stream that can contain both text and tool calls.
     *
     * @param props - Configuration for content generation
     * @param props.messages - Array of message objects for chat-like interactions
     * @param props.config - Configuration parameters for text generation
     * @param props.tools - Optional array of tools that the model may call
     * @returns A content stream iterator for the generated content (text + tool calls)
     * @throws InworldError if messages are not provided, or generation fails
     */
    async generateContentStream(props) {
        if (!props.messages) {
            throw new common_1.InworldError('Messages must be provided for LLM content generation');
        }
        let generateStatus;
        let messages = null;
        let tools = null;
        try {
            messages = messages_1.Messages.fromLLMMessageInterfaces(props.messages);
            if (props.tools && props.tools.length > 0) {
                tools = messages_2.Tools.fromToolInterfaces(props.tools);
                generateStatus =
                    await expose_binary_1.LLMInterfaceFunctions.generateContentFromMessagesWithTools(this.llm, messages.getExternal(), new text_generator_1.TextGenerationConfig(props.config).getExternal(), tools.getExternal());
            }
            else {
                generateStatus = await this.buildGenerateStatus({
                    messages: messages,
                    config: props.config,
                });
            }
            if (!expose_binary_1.LLMInterfaceFunctions.isOK(generateStatus)) {
                throw new common_1.InworldError('Failed to generate content', (0, helpers_1.getStatus)(generateStatus));
            }
            return this.contentStreamIterator(expose_binary_1.InputStreamFunctions.get(generateStatus), messages);
        }
        catch (error) {
            messages === null || messages === void 0 ? void 0 : messages.destroy();
            tools === null || tools === void 0 ? void 0 : tools.destroy();
            throw error;
        }
        finally {
            if (generateStatus) {
                expose_binary_1.InputStreamFunctions.delete(generateStatus);
            }
        }
    }
    /**
     * Returns the external reference to the native LLM implementation.
     *
     * @returns External reference object
     * @internal
     */
    getExternal() {
        return this.llm;
    }
    /**
     * Cleans up resources associated with this LLM.
     */
    destroy() {
        if (this.llm) {
            expose_binary_1.LLMInterfaceFunctions.delete(this.llm);
            this.llm = null;
        }
        if (this.factory) {
            expose_binary_1.LLMFactoryFunctions.delete(this.factory);
            this.factory = null;
        }
    }
    /**
     * Builds the generate status object for text generation.
     *
     * @param props - Configuration for text generation
     * @returns Promise resolving to the generate status object
     * @private
     */
    async buildGenerateStatus(props) {
        let generateStatus;
        let textGenerationConfig = null;
        try {
            textGenerationConfig = new text_generator_1.TextGenerationConfig(props.config);
            if (props.prompt) {
                generateStatus = await expose_binary_1.LLMInterfaceFunctions.generateContentFromPrompt(this.llm, props.prompt, textGenerationConfig.getExternal());
            }
            else if (props.messages) {
                generateStatus =
                    await expose_binary_1.LLMInterfaceFunctions.generateContentFromMessages(this.llm, props.messages.getExternal(), textGenerationConfig.getExternal());
            }
            else {
                throw new common_1.InworldError('Either prompt or messages must be provided for LLM generation');
            }
        }
        finally {
            textGenerationConfig === null || textGenerationConfig === void 0 ? void 0 : textGenerationConfig.destroy();
        }
        return generateStatus;
    }
    /**
     * Creates a content stream iterator for the generated content.
     *
     * @param inputStream - External reference to the input stream
     * @param messages - Optional messages object
     * @returns Content stream iterator object
     * @private
     */
    contentStreamIterator(inputStream, messages) {
        const contentStream = new content_1.ContentStream(inputStream, () => {
            expose_binary_1.InputStreamFunctions.deleteStream(inputStream);
            messages === null || messages === void 0 ? void 0 : messages.destroy();
        });
        return {
            getStream() {
                return contentStream.getStream();
            },
            async next() {
                return contentStream.next();
            },
            toTextResponse(props) {
                return contentStream.toTextResponse(props);
            },
            toTTSOutputResponse(props) {
                return contentStream.toTTSOutputResponse(props);
            },
        };
    }
}
exports.LLM = LLM;
