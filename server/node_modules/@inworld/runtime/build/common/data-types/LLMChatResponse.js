"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.LLMChatResponse = void 0;
const expose_binary_1 = require("../../expose_binary");
const stream_1 = require("../stream");
const messages_1 = require("../vector/messages");
const BaseData_1 = require("./BaseData");
/**
 * LLM chat response for graph processing.
 *
 * Handles both streaming and complete responses from language model interactions.
 */
class LLMChatResponse extends BaseData_1.BaseData {
    /**
     * @internal
     * Creates a new LLMChatResponse instance.
     *
     * @param {ExternalReference} external - External reference to the native LLM chat response implementation
     */
    constructor(external) {
        super(external);
    }
    /**
     * @internal
     * Creates a new LLMChatResponse instance from an external reference.
     *
     * @param {ExternalReference} external - The external reference to create the instance from
     * @returns {LLMChatResponse | null} A new LLMChatResponse instance or null if invalid
     */
    static fromExternal(external) {
        const baseData = expose_binary_1.LLMChatResponseFunctions.toBaseData(external);
        if (baseData && expose_binary_1.LLMChatResponseFunctions.isValid(baseData)) {
            return new LLMChatResponse(baseData);
        }
        return null;
    }
    /**
     * Checks if this response is streaming.
     *
     * @returns {boolean} True if the response is streaming
     */
    isStreaming() {
        return expose_binary_1.LLMChatResponseFunctions.isStreaming(this.external);
    }
    /**
     * Checks if this response has complete content.
     *
     * @returns {boolean} True if the response has complete content
     */
    hasCompleteContent() {
        return expose_binary_1.LLMChatResponseFunctions.hasCompleteContent(this.external);
    }
    /**
     * Gets the streaming data for streaming responses.
     *
     * @returns {ContentStream | null} ContentStream for streaming responses or null for non-streaming
     */
    getContentStream() {
        if (this.isStreaming()) {
            return stream_1.ContentStream.fromExternal(expose_binary_1.LLMChatResponseFunctions.getStream(this.external));
        }
        return null;
    }
    /**
     * For non-streaming, we can still use a stream to get the content
     * The stream will contain both text and tool calls in a single chunk
     *
     * @returns {ContentInterface} The LLMChatResponse interface object
     */
    getContent() {
        const externalContent = expose_binary_1.LLMChatResponseFunctions.getContent(this.external);
        const text = expose_binary_1.ContentFunctions.getContent(externalContent);
        let toolCalls = [];
        if (!this.isStreaming() && this.hasCompleteContent()) {
            const toolCallsVector = expose_binary_1.ContentFunctions.getToolCalls(externalContent);
            toolCalls = messages_1.ToolCalls.getToolCallInterfaces(toolCallsVector);
        }
        return Object.assign({ content: text }, (toolCalls.length > 0 && { toolCalls: toolCalls }));
    }
    destroy() {
        super.destroy();
    }
}
exports.LLMChatResponse = LLMChatResponse;
