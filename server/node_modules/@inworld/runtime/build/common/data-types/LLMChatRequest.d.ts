import { LLMMessageInterface, ToolChoiceInterface, ToolInterface } from '../vector/messages';
import { BaseData } from './BaseData';
import { ExternalReference } from './common';
/**
 * Structured chat request for language model interactions.
 *
 * Contains messages, optional tools, tool choice configuration, and response format.
 */
export interface LLMChatRequestInterface {
    /** Array of messages in the conversation history. */
    messages: LLMMessageInterface[];
    /** Optional array of tools that the language model can call. */
    tools?: ToolInterface[];
    /** Optional configuration for tool selection behavior. */
    toolChoice?: ToolChoiceInterface;
    /** Optional specification for the response format. */
    responseFormat?: ResponseFormatName;
}
/**
 * Class for handling chat messages in graphs.
 * Extends BaseData with chat message-specific functionality.
 */
export declare class LLMChatRequest extends BaseData {
    private messages;
    private tools?;
    private toolChoice?;
    private responseFormat?;
    constructor(messagesOrExternalOrInterface: LLMMessageInterface[] | ExternalReference | LLMChatRequestInterface);
    /**
     * Creates an OptionalToolChoice from toolChoice value
     *
     * @param {ToolChoiceInterface} [toolChoice] - Optional tool choice configuration
     * @returns {ExternalReference} External reference to the optional tool choice
     * @private
     */
    private static createOptionalToolChoice;
    /**
     * @internal
     * Creates a new LLMChatRequest instance from an external reference.
     *
     * @param {ExternalReference} external - The external reference to create the instance from
     * @returns {LLMChatRequest | null} A new LLMChatRequest instance or null if invalid
     */
    static fromExternal(external: ExternalReference): LLMChatRequest | null;
    /**
     * Gets the LLMChatRequest data as an interface.
     * If external is set, it will parse the external reference,
     * else it will use the stored properties
     *
     * @returns {LLMChatRequestInterface} The LLMChatRequest interface object
     */
    getLLMChatRequest(): LLMChatRequestInterface;
    /**
     * Cleans up resources used by the LLMChatRequest instance.
     */
    destroy(): void;
}
/** Response format name. */
export declare enum ResponseFormatName {
    /** Plain text response format */
    Text = "text",
    /** JSON response format */
    Json = "json",
    /** JSON Schema response format */
    JsonSchema = "json_schema"
}
