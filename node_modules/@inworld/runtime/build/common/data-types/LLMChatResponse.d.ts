import { ContentStream } from '../stream';
import { ContentInterface } from '../vector/messages';
import { BaseData } from './BaseData';
import { ExternalReference } from './common';
/**
 * LLM chat response for graph processing.
 *
 * Handles both streaming and complete responses from language model interactions.
 */
export declare class LLMChatResponse extends BaseData {
    /**
     * @internal
     * Creates a new LLMChatResponse instance.
     *
     * @param {ExternalReference} external - External reference to the native LLM chat response implementation
     */
    constructor(external: ExternalReference);
    /**
     * @internal
     * Creates a new LLMChatResponse instance from an external reference.
     *
     * @param {ExternalReference} external - The external reference to create the instance from
     * @returns {LLMChatResponse | null} A new LLMChatResponse instance or null if invalid
     */
    static fromExternal(external: ExternalReference): LLMChatResponse | null;
    /**
     * Checks if this response is streaming.
     *
     * @returns {boolean} True if the response is streaming
     */
    isStreaming(): boolean;
    /**
     * Checks if this response has complete content.
     *
     * @returns {boolean} True if the response has complete content
     */
    hasCompleteContent(): boolean;
    /**
     * Gets the streaming data for streaming responses.
     *
     * @returns {ContentStream | null} ContentStream for streaming responses or null for non-streaming
     */
    getContentStream(): ContentStream | null;
    /**
     * For non-streaming, we can still use a stream to get the content
     * The stream will contain both text and tool calls in a single chunk
     *
     * @returns {ContentInterface} The LLMChatResponse interface object
     */
    getContent(): ContentInterface;
    destroy(): void;
}
